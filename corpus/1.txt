
Задача суммаризации текстов (автореферирование) - одна из ключевых, широко обсуждаемых задач NLP. Она состоит в сжатии больших объемов текста до связного краткого содержания, отражающего только основные идеи.
Сейчас нам доступно огромное количество текстовой информации по любой теме. Чтобы сократить время на ознакомление с интересующей информацией используют алгоритмы суммаризации текстов. Их задача выделить из потока текстовых данных главные идеи и создать на их основе сокращенный читаемый текст. Так, суммаризация может помочь понять содержание той или иной научной статьи, получить свежие выдержки из новостей или облегчить понимание юридического заключения или финансового отчета. Автореферирование актуально практически во всех областях, так как существенно сокращает время чтения.

Экономия времени на чтении актуальна и ежегодно публикуется множество статей, описывающих новые методы и улучшения существующих решений. Наибольший успех имеют нейронные сети, но есть и более простые и быстрые подходы, использующиеся в большинстве статей в качестве исходной точки для сравнения качества. Оптимального и универсального решения задачи автоматической суммаризации еще не найдено. Ниже мы перечислили стандартные подходы и их ограничения. Также, мы предложили свое обобщение одного из алгоритмов, решающее некоторые из его проблем.
Все алгоритмы автоматической суммаризации делятся на два класса - extractive summarization и abstractive summarization.
Extractive summarization - техника, позволяющая выделять из текста наиболее качественно описывающие основные идеи текста предложения, не думая о том, будет ли получившийся текст связным и читаемым. Ниже подробно описаны походы и классические алгоритмы, применяемые для решения задачи.

Например, если мы обработаем первый параграф статьи Википедии о компании Apple, можем получить такой результат: "Apple — американская корпорация, производитель персональных и планшетных компьютеров. В тот же день компания стала самой дорогой публичной компанией за всю историю."

Abstractive summarization - техника, которая основана на выделении основных идей из текста и последующей генерации краткого содержания с нуля на основе выданных идей.

На данный момент все подобные алгоритмы abstractive summarization основаны на нейронных сетях, а значит, для их обучения необходимо большое количество пар текст-ручная суммаризация. Несмотря на то, что подходы из данного класса демонстрируют лучшее качество и более приближены к решению исходной задачи, часто нехватка размеченных данных и существенное время обучения препятствуют использованию алгоритмов abstractive summarization. В основном, используются encoder-decoder архитектуры, использующие рекуррентные нейронные сети и архитектуру Transformer. Наиболее распространенная готовая реализация представлена в пакете OpenNMT-tf.
Алгоритмы extractive summarization, как правило, достаточно просты в реализации и основаны на простых эвристиках. Большинство из них не требуют никакой обучающей выборки, что делает их самыми привлекательными для использования на реальных данных. Чаще всего, каждому предложению из текста присваивается вес "важности", затем выбираются несколько наиболее "важных".

Один из первых алгоритмов был основан на технике LSA (Latent Semantic Analysis). В основе лежит построение матрицы (слова x предложения), а затем применение сингулярного разложения для этой матрицы. Так, для каждого слова можно получить необходимый вес, а в суммаризацию включать те предложения, чья суммарная "важность" слов наибольшая.

Семейство вероятностных алгоритмов оперирует с распределениями слов в тексте: алгоритм KLSum, используя KL дивергенцию как меру расстояния между вероятностными распределениями, ищет подмножество предложений текста, распределение слов в которых ближе всего к распределению слов в исходном тексте. Другой распространенный алгоритм этого семейства основан на понятии энтропии. Энтропия вероятностного распределения показывает его неопределенность. Жадный алгоритм добавляет предложения в суммаризацию до тех пор, пока энтропия суммаризации уменьшается, действительно, нужно, чтобы полученное краткое содержание было достаточно определенным и не включало ничего лишнего.

Семейство графовых алгоритмов extractive summarization присваивает тем больший вес предложениям, чем больше других предложений похожи на него. Действительно, основные идеи часто повторяются несколько раз в тексте, но в перефразированной форме. Алгоритмы называются графовыми, так как предложения выступают в роли вершин, а ребра проводятся, если предложения достаточно близки. Для определения веса предложения в алгоритме TextRank используется модифицированный алгоритм PageRank от Google. В алгоритме LexRank проводится кластеризация предложений с метрикой, основанной на смысловой близости. Так выделяются кластеры предложений, описывающих одну идею. Далее из каждого кластера выбирается центроид (самое близкое к центру кластера предложение), которое попадает в финальную суммаризацию. Объединение этих идей - сначала кластеризация предложений и построение графа на основе полученной кластеризации и запуск PageRank для расчета весов, получило название ClusterRank.

Для того, чтобы оценить качество работы алгоритма суммаризации используются два метода: оценка «на глаз» и оценивание с помощью специальной ROUGE-метрики. На текущий момент графовые алгоритмы лидируют по качеству среди алгоритмов extractive summarization. Однако, большинство графовых алгоритмов extractive summarization обладают недостатками, которые заметны при запуске алгоритмов на разных типах текстов. Мы приведем пять самых существенных.
Метрика близости. Действительно, универсальной метрики «схожести» двух предложений по смыслу не существует. В оригинальной статье про TextRank предлагается объявлять предложения похожими, если доля общих слов в них достаточно большая. Такая метрика показывает низкое качество, и вместо нее используются техника семантических векторных представлений слов. Однако, формирование качественного векторного представления предложений на основе векторов слов - не самая простая задача.

Длинные и общие предложения. Часто, чем длиннее предложение, тем на большее число других предложений текста оно похоже. Алгоритм будет выделять такие предложения в качестве ключевых, что существенно увеличивает общий размер полученной текстовой выдержки. Аналогичная проблема наблюдается с вводными и общими предложениями, они тоже часто выделяются, даже когда основными не являются. В качестве решения мы предлагаем сегментировать длинные предложения на части, а затем подавать на вход алгоритму отельные сегменты, а не предложения целиком.

Длинные тексты. Чем длиннее текст, тем больше в нем шумовых предложений, которые не несут никакую идею. Такие шумовые предложения часто похожи друг на друга, и алгоритм выделяет несколько из них в финальную суммаризацию. Особенно это заметно для книг и, особенно, художественной литературы, где, в целом мало предложений, отражающих основные идеи.

Перифразы. Если в тексте присутствуют два одинаковых по смыслу предложения, отражающих ключевую идею (например, тезис и заключение), то алгоритм включит в суммаризацию оба таких предложения. Действительно, на них обоих похожа большая часть текста и у алгоритма нет оснований убрать одно из них. Это негативно сказывается как на размере получившейся суммаризации, так и на ее качестве. Больше всего такая проблема заметна при суммаризации научных и научно-популярных источников, например, Википедии. В качестве потенциального решения проблемы можно предложить итератитивную суммаризацию. Сначала формируется суммаризация из исходного текста, а затем получившееся краткое содержание повторно сжимается до тех пор, пока она не перестанет меняться. Так, из всех похожих предложений останутся только самые точные представители.

Формализм. Чем формальнее текст, тем больше предложения похожи друг на друга и тем сложнее выделить из них ключевые. Это больше всего заметно на суммаризации юридических текстов. Стиль настолько формален, что все предложения очень похожи воспринимаются алгоритмом очень похожими, в связи с чем суммаризация получается очень зашумленная.

Типы текстов, на которых не заметно таких проблем - новости. Именно поэтому в статьях обычно проверяют качество суммаризации именно на датасетах новостей. Научная и научно-популярная литература тоже могут быть достаточно качественно суммаризированы как и финансовые и бизнес статьи из различных журналов. Генерировать краткие содержания книг сложнее всего - почти все проблемы можно найти в таких типах текста.

Для нахождения оптимального алгоритма для автореферирования требуются исследования. В рамках написания этой статьи мы реализовали наши идеи по улучшению алгоритма extractive summarization TextRank. Ниже расположена интерактивная демонстрация работы получившегося алгоритма.

Наша реализация суммаризации основана на алгоритме TextRank, при этом мы постарались решить описанные выше проблемы и сделали его болееэффективным. Данная прототип работает только для русского языка для текстов произвольного размера.

Мы строим векторные представления предложений, усредняя векторы FastText для каждого слова. Мы используем косинусное расстояние между полученными векторами как меру близости. Отдельно обрабатываются случаи похожих предложений и длинных текстов.